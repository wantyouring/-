model1,2 : state3개 붙여 history로 학습.
결과 : avg q max 계속 감소.

model3 : state1개씩 학습.
이어서 학습하기!!!!!!!!!!!!!!
약1시간 + 2:00~11:30 학습.
=> avg q max 0.5~0.6사이에서 더 안올라감.

model4 : learning rate 줄여보자. (0.001 -> 0.00025)
epsilon_min (0.01 -> 0.1)
12000epi부터 0.1 -> 0.01로 다시 줄이기.

5600epi까지 학습.
12000epi까지 학습.
16500epi까지 학습.

@model5 : fully connected (1024,256,3) layer로 model변경.
--학습중--(4900epi까지 학습)
breakout예제 168만 params. (1024,256,3) params 39만.
kernel_initializer he_uniform 없앰. 
12000epi까지 learning rate=0.00025까지 돌리고 0.001로 올림. epsilon min 0.1로 올림. epsilon decay 0.99로 줄임. ==> 잘 안됨. 원상복귀해서 이어서 learning

model6 : (2048,512,3) layer로 model 변경.=> 오래 걸릴 것 같음. 자기전에 돌리자. 우선 800epi까지 학습함.

model7 : (256,256,3) layer로 model 변경. layer 간단하게. 우선 모델 2600epi까지 학습. 이어서 더 돌려보기.

---------------------------
model5가 가장 괜찮은 결과. 조금 학습하였음. 그러나 좋은 결과는 x.

-------------------
원래 breakout-ram버전에서 lr 줄이고 life패널티 -100주기. => model2 history사용한 model에서 다시 학습해봄. => 잘 학습되나 reward의 상하폭이 frame skip있는 환경보다 큼. 10000epi까지 학습해봄.
